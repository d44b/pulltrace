{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Pulltrace","text":"<p>Real-time Kubernetes image pull progress monitor.</p> <p>Pulltrace gives you live visibility into image pulls happening across your cluster nodes \u2014 layers, download speed, and ETA \u2014 in a single web UI.</p>"},{"location":"#what-it-does","title":"What It Does","text":"<p>When you deploy a new image to your cluster, Kubernetes schedules pods and nodes begin pulling image layers from the registry. This process is invisible by default: <code>kubectl get pods</code> shows <code>ContainerCreating</code> with no detail.</p> <p>Pulltrace fills that gap. It runs a lightweight DaemonSet agent on each node that reads from the containerd socket, and a central server that aggregates the data and streams it to a browser UI.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Per-layer progress \u2014 see which layers are downloading, their size, and download speed</li> <li>ETA \u2014 estimated time remaining based on current download rate</li> <li>Multi-node \u2014 all nodes in the cluster visible in one view</li> <li>Pod correlation \u2014 image references linked to pod names via the Kubernetes API</li> <li>Prometheus metrics \u2014 pull counts, durations, bytes, and error rates</li> <li>Single <code>helm install</code> \u2014 deploys as a DaemonSet + Deployment with a standard Helm chart</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>See the Installation page for the full install guide.</p> <pre><code>helm repo add pulltrace https://d44b.github.io/pulltrace/charts\nhelm repo update\nhelm install pulltrace pulltrace/pulltrace -n pulltrace --create-namespace\n</code></pre>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Kubernetes 1.28+ with containerd runtime</li> <li>Helm 3</li> </ul>"},{"location":"architecture/","title":"Architecture","text":"<p>Pulltrace has three components: an agent DaemonSet, a central server, and a React web UI.</p>"},{"location":"architecture/#data-flow","title":"Data Flow","text":"<pre><code>flowchart LR\n    subgraph node[\"Each Kubernetes Node\"]\n        CT[containerd\\nSocket]\n        AG[pulltrace-agent\\nDaemonSet]\n        CT --&gt;|content.ListStatuses| AG\n    end\n\n    AG --&gt;|POST /api/v1/report\\nAgentReport JSON| SV\n\n    subgraph cluster[\"Kubernetes Cluster\"]\n        SV[pulltrace-server\\nDeployment]\n        K8S[Kubernetes API\\npod/event watcher]\n        SV --&gt;|watch pods+events| K8S\n    end\n\n    SV --&gt;|GET /api/v1/events\\nSSE PullEvent stream| UI[Web UI\\nReact + Vite]\n    SV --&gt;|GET /metrics\\nPrometheus| PROM[Prometheus]</code></pre>"},{"location":"architecture/#components","title":"Components","text":""},{"location":"architecture/#agent-daemonset","title":"Agent (DaemonSet)","text":"<p>One agent pod runs on every node. It connects to the local containerd socket (<code>/run/containerd/containerd.sock</code> by default) and calls <code>content.ListStatuses</code> to enumerate active image layer downloads. Every second (configurable via <code>PULLTRACE_REPORT_INTERVAL</code>) it sends an <code>AgentReport</code> JSON payload to the server over HTTP.</p>"},{"location":"architecture/#server-deployment","title":"Server (Deployment)","text":"<p>The server is the single aggregation point. It:</p> <ol> <li>Receives <code>AgentReport</code> payloads from all agents via <code>POST /api/v1/report</code></li> <li>Correlates image references with pod names by watching the Kubernetes pod and event APIs</li> <li>Maintains an in-memory pull state map with a configurable TTL (<code>PULLTRACE_HISTORY_TTL</code>, default 30m)</li> <li>Streams <code>PullEvent</code> updates to connected browsers via Server-Sent Events on <code>GET /api/v1/events</code></li> <li>Exposes Prometheus metrics on a separate port (<code>PULLTRACE_METRICS_ADDR</code>, default <code>:9090</code>)</li> </ol>"},{"location":"architecture/#web-ui","title":"Web UI","text":"<p>A React single-page application served by the server at the root path. It establishes an SSE connection to <code>/api/v1/events</code> on load and renders live pull progress \u2014 per-node, per-image, per-layer \u2014 with ETA and download speed.</p>"},{"location":"architecture/#api","title":"API","text":"Endpoint Method Description <code>/api/v1/report</code> POST Agent reports pull state; body is <code>AgentReport</code> JSON <code>/api/v1/events</code> GET SSE stream of <code>PullEvent</code> messages for the UI <code>/api/v1/pulls</code> GET Current pull state snapshot (used by UI on initial load) <code>/metrics</code> GET Prometheus metrics (served on <code>PULLTRACE_METRICS_ADDR</code>)"},{"location":"configuration/","title":"Configuration","text":"<p>Pulltrace is configured entirely through environment variables. The Helm chart sets all required values automatically; the tables below are for reference when deploying outside Helm or overriding defaults.</p>"},{"location":"configuration/#server","title":"Server","text":"<p>The server is the central aggregator. It receives reports from agents, watches the Kubernetes API for pod correlation, and serves the web UI and SSE event stream.</p> Variable Type Default Description <code>PULLTRACE_HTTP_ADDR</code> string <code>:8080</code> HTTP listen address for the API and UI <code>PULLTRACE_METRICS_ADDR</code> string <code>:9090</code> Prometheus metrics listen address <code>PULLTRACE_LOG_LEVEL</code> string <code>info</code> Log level: <code>debug</code>, <code>info</code>, <code>warn</code>, <code>error</code> <code>PULLTRACE_AGENT_TOKEN</code> string (empty) Shared token for agent authentication (optional; leave empty to disable auth) <code>PULLTRACE_WATCH_NAMESPACES</code> string (empty \u2014 all) Comma-separated namespaces for pod/event correlation; empty means watch all namespaces <code>PULLTRACE_HISTORY_TTL</code> duration <code>30m</code> How long completed pulls remain visible in the UI"},{"location":"configuration/#agent","title":"Agent","text":"<p>One agent DaemonSet pod runs on each node. It polls the local containerd socket and reports image pull progress to the server.</p> Variable Type Default Description <code>PULLTRACE_NODE_NAME</code> string (required) Kubernetes node name; injected automatically via <code>fieldRef: spec.nodeName</code> <code>PULLTRACE_SERVER_URL</code> string (required) URL of the Pulltrace server (e.g. <code>http://pulltrace-server:8080</code>) <code>PULLTRACE_CONTAINERD_SOCKET</code> string <code>/run/containerd/containerd.sock</code> Host path to the containerd gRPC socket <code>PULLTRACE_LOG_LEVEL</code> string <code>info</code> Log level: <code>debug</code>, <code>info</code>, <code>warn</code>, <code>error</code> <code>PULLTRACE_AGENT_TOKEN</code> string (empty) Bearer token sent to the server; must match <code>PULLTRACE_AGENT_TOKEN</code> on the server if set <code>PULLTRACE_REPORT_INTERVAL</code> duration <code>1s</code> How often the agent polls containerd and sends a report to the server"},{"location":"configuration/#helm-values","title":"Helm Values","text":"<p>When using the Helm chart, set environment variables via <code>values.yaml</code> overrides:</p> <pre><code>server:\n  env:\n    PULLTRACE_LOG_LEVEL: debug\n    PULLTRACE_HISTORY_TTL: 60m\n\nagent:\n  env:\n    PULLTRACE_REPORT_INTERVAL: 2s\n</code></pre> <p>See <code>charts/pulltrace/values.yaml</code> in the repository for the full values reference.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to Pulltrace.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<p>See CONTRIBUTING.md in the repository root for:</p> <ul> <li>Development prerequisites (Go 1.22+, the Docker workaround for local machines with older Go)</li> <li>Build instructions (<code>make</code> and <code>docker build</code>)</li> <li>PR guidelines</li> </ul>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":"<p>Open an issue on GitHub. Include your Kubernetes version, containerd version, and the Pulltrace version (<code>helm list -n pulltrace</code>).</p>"},{"location":"contributing/#architecture-overview","title":"Architecture Overview","text":"<p>Before contributing, reading the Architecture page and the Architecture Decision Records will give you context on why key design choices were made.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.28+ with containerd runtime (CRI-O is not supported \u2014 Pulltrace reads the containerd gRPC socket directly)</li> <li>Helm 3</li> <li>A namespace with the <code>pod-security.kubernetes.io/enforce=privileged</code> label \u2014 the agent DaemonSet mounts the host containerd socket via <code>hostPath</code>, which requires a privileged pod security profile</li> </ul>"},{"location":"installation/#install","title":"Install","text":"<pre><code># 1. Add the Pulltrace Helm repository\nhelm repo add pulltrace https://d44b.github.io/pulltrace/charts\nhelm repo update\n\n# 2. Create the namespace with the required pod security label\nkubectl create namespace pulltrace\nkubectl label namespace pulltrace \\\n  pod-security.kubernetes.io/enforce=privileged --overwrite\n\n# 3. Install Pulltrace\nhelm install pulltrace pulltrace/pulltrace \\\n  -n pulltrace\n</code></pre>"},{"location":"installation/#access-the-ui","title":"Access the UI","text":"<pre><code>kubectl port-forward -n pulltrace svc/pulltrace-server 8080:8080\n</code></pre> <p>Open http://localhost:8080 in your browser.</p>"},{"location":"installation/#upgrade","title":"Upgrade","text":"<pre><code>helm repo update\nhelm upgrade pulltrace pulltrace/pulltrace -n pulltrace\n</code></pre>"},{"location":"installation/#uninstall","title":"Uninstall","text":"<pre><code>helm uninstall pulltrace -n pulltrace\n</code></pre>"},{"location":"installation/#see-also","title":"See Also","text":"<ul> <li>Configuration \u2014 environment variables for server and agent</li> <li>Architecture \u2014 how Pulltrace works</li> </ul>"},{"location":"known-limitations/","title":"Known Limitations","text":""},{"location":"known-limitations/#containerd-only","title":"containerd Only","text":"<p>Pulltrace reads the containerd gRPC socket directly. Nodes using CRI-O or Docker Engine (without containerd) are not supported.</p>"},{"location":"known-limitations/#in-memory-state-only","title":"In-Memory State Only","text":"<p>Pull history is stored in-memory on the server with a configurable TTL (default 30 minutes). Restarting the server clears all pull history. There is no persistence layer.</p>"},{"location":"known-limitations/#single-cluster","title":"Single Cluster","text":"<p>One Pulltrace installation monitors one Kubernetes cluster. Multi-cluster federation is not supported in v0.1.0.</p>"},{"location":"known-limitations/#no-ui-authentication","title":"No UI Authentication","text":"<p>The web UI is read-only and has no built-in authentication. It is designed for use behind an ingress controller with auth, within a private cluster network, or accessed via <code>kubectl port-forward</code>. Do not expose the UI directly to the public internet without an authentication proxy.</p>"},{"location":"known-limitations/#layer-events-not-streamed","title":"Layer Events Not Streamed","text":"<p>The server maintains per-layer progress state internally, but SSE events for individual layer start/progress/completion events are not emitted in v0.1.0. Layer data is visible in the UI on each pull update cycle.</p>"},{"location":"prometheus/","title":"Prometheus Metrics","text":"<p>Pulltrace exposes Prometheus metrics on a separate port (default <code>:9090</code>, configured via <code>PULLTRACE_METRICS_ADDR</code>).</p>"},{"location":"prometheus/#scrape-configuration","title":"Scrape Configuration","text":"<pre><code>scrape_configs:\n  - job_name: pulltrace\n    static_configs:\n      - targets: [\"pulltrace-server:9090\"]\n</code></pre> <p>Or using Kubernetes service discovery with a <code>prometheus.io/scrape: \"true\"</code> annotation (the Helm chart sets this by default).</p>"},{"location":"prometheus/#metrics-reference","title":"Metrics Reference","text":"Metric Type Description <code>pulltrace_pulls_active</code> Gauge Image pulls currently in progress across all nodes <code>pulltrace_pulls_total</code> Counter Total image pulls observed since server startup <code>pulltrace_pull_duration_seconds</code> Histogram Pull duration in seconds (buckets: 1s, 5s, 10s, 30s, 1m, 2m, 5m, 10m) <code>pulltrace_pull_bytes_total</code> Counter Total bytes downloaded across all pulls since server startup <code>pulltrace_pull_errors_total</code> Counter Pulls that completed with a non-empty error field <code>pulltrace_agent_reports_total</code> Counter Total agent report payloads received by the server <code>pulltrace_sse_clients_active</code> Gauge Number of active SSE connections (browser UI clients)"},{"location":"prometheus/#example-alert","title":"Example Alert","text":"<pre><code>groups:\n  - name: pulltrace\n    rules:\n      - alert: ImagePullErrors\n        expr: increase(pulltrace_pull_errors_total[5m]) &gt; 0\n        for: 0m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Image pull errors detected\"\n          description: \"{{ $value }} pull error(s) in the last 5 minutes\"\n</code></pre>"},{"location":"adr/001-runtime-containerd/","title":"ADR-001: Use containerd Content Store API for Progress Tracking","text":""},{"location":"adr/001-runtime-containerd/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/001-runtime-containerd/#date","title":"Date","text":"<p>2025-01-10</p>"},{"location":"adr/001-runtime-containerd/#context","title":"Context","text":"<p>Kubernetes does not expose image pull progress through any standard API. The CRI (Container Runtime Interface) provides <code>PullImage</code> as a synchronous RPC that returns only when the pull completes -- there is no streaming progress callback. To provide real-time pull visibility, we need to go below the CRI layer and interact with the container runtime directly.</p> <p>The two dominant container runtimes in production Kubernetes clusters are:</p> <ul> <li>containerd -- used by default in EKS, GKE, AKS, k3s, kind, and most managed Kubernetes distributions.</li> <li>CRI-O -- used primarily in OpenShift and some bare-metal deployments.</li> </ul> <p>Each runtime has different internal APIs for observing download progress.</p>"},{"location":"adr/001-runtime-containerd/#containerd-approach","title":"containerd approach","text":"<p>containerd v2 exposes a content store API via gRPC. When an image is being pulled, each layer is downloaded as an \"ingest\" in the content store. The <code>content.ListStatuses</code> API returns all active ingests with their <code>Offset</code> (bytes downloaded) and <code>Total</code> (expected size) fields, updated in real time.</p>"},{"location":"adr/001-runtime-containerd/#cri-o-approach","title":"CRI-O approach","text":"<p>CRI-O uses the <code>containers/image</code> library internally. Progress tracking would require either parsing CRI-O's log output or using CRI-O-specific extension APIs, which are less stable and less documented.</p>"},{"location":"adr/001-runtime-containerd/#decision","title":"Decision","text":"<p>Pulltrace will use the containerd v2 content store API (<code>content.ListStatuses</code>) as its primary mechanism for tracking image pull progress.</p> <p>The agent binary will:</p> <ol> <li>Connect to the containerd gRPC socket (default: <code>/run/containerd/containerd.sock</code>).</li> <li>Periodically call <code>content.ListStatuses</code> to enumerate active ingests.</li> <li>Correlate ingests with image references using the <code>content.Info</code> metadata.</li> <li>Compute per-layer and per-image progress, rates, and ETAs.</li> </ol>"},{"location":"adr/001-runtime-containerd/#rationale","title":"Rationale","text":"<ul> <li>Market share. containerd is the most widely deployed Kubernetes runtime. Supporting it first maximizes reach.</li> <li>API quality. The content store <code>ListStatuses</code> API provides exactly the data we need: bytes downloaded and total size per layer, updated in real time.</li> <li>Stability. The containerd v2 content API is a published, stable gRPC service with protobuf definitions.</li> <li>Simplicity. A single gRPC call returns all active downloads on the node. No log parsing or filesystem watching is required.</li> <li>Extensibility. The agent architecture is pluggable. A CRI-O backend can be added later behind the same internal interface without changing the server or UI.</li> </ul>"},{"location":"adr/001-runtime-containerd/#consequences","title":"Consequences","text":"<ul> <li>Pulltrace will only work on nodes running containerd v2. Nodes using CRI-O, Docker (dockershim), or other runtimes will not report pull progress.</li> <li>The agent requires access to the containerd socket, which means it needs a host path volume mount. This has security implications documented in SECURITY.md.</li> <li>Total layer sizes may not be known until the image manifest is resolved. The agent must handle the case where <code>Total</code> is zero and mark the size as unknown (<code>totalKnown: false</code>).</li> <li>If containerd changes its content store API in a future major version, the agent will need to be updated.</li> </ul>"},{"location":"adr/002-agent-server-protocol/","title":"ADR-002: HTTP POST from Agent to Server (Not gRPC)","text":""},{"location":"adr/002-agent-server-protocol/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/002-agent-server-protocol/#date","title":"Date","text":"<p>2025-01-10</p>"},{"location":"adr/002-agent-server-protocol/#context","title":"Context","text":"<p>The Pulltrace agent runs on every node and needs to send pull progress data to the central server. We need a communication protocol that is:</p> <ul> <li>Reliable across node-to-pod networking in Kubernetes.</li> <li>Simple to implement and debug.</li> <li>Resilient to transient network failures.</li> <li>Efficient enough for periodic small payloads (a few KB every 2 seconds per node).</li> </ul> <p>The two main options considered were:</p> <ol> <li>HTTP POST -- Agent periodically POSTs a JSON snapshot of all active pulls to the server.</li> <li>gRPC streaming -- Agent opens a bidirectional or server-streaming gRPC connection to the server.</li> </ol>"},{"location":"adr/002-agent-server-protocol/#decision","title":"Decision","text":"<p>Use HTTP POST for agent-to-server communication and Server-Sent Events (SSE) for server-to-UI streaming.</p>"},{"location":"adr/002-agent-server-protocol/#agent-to-server","title":"Agent to Server","text":"<ul> <li>The agent sends an <code>AgentReport</code> JSON payload via <code>POST /api/v1/report</code> at a configurable interval (default: 2 seconds).</li> <li>Each report is a full snapshot of all active pulls on that node, not a delta.</li> <li>The server is stateless with respect to agent connections -- it merges the latest report from each node.</li> </ul>"},{"location":"adr/002-agent-server-protocol/#server-to-ui","title":"Server to UI","text":"<ul> <li>The server exposes <code>GET /api/v1/events</code> as an SSE endpoint.</li> <li>Each event is a <code>PullEvent</code> JSON object representing a state change.</li> <li>Clients can reconnect at any time and receive the current state via <code>GET /api/v1/pulls</code>.</li> </ul>"},{"location":"adr/002-agent-server-protocol/#rationale","title":"Rationale","text":"<ul> <li>Simplicity. HTTP POST requires no proto compilation, no streaming state management, and no bidirectional connection handling. The agent is a simple loop: snapshot, serialize, POST, sleep.</li> <li>Debuggability. Reports can be inspected with <code>curl -X POST</code> and responses are plain JSON. No special tooling is needed.</li> <li>Resilience. If a POST fails, the agent simply retries on the next interval with fresh data. There is no connection state to recover. Full-snapshot semantics mean the server always has the latest state without needing to replay missed deltas.</li> <li>Adequate performance. An <code>AgentReport</code> for a node pulling 5 images with 10 layers each is approximately 2-3 KB of JSON. At a 2-second interval, this is negligible network overhead.</li> <li>SSE for downstream. SSE is natively supported by browsers (EventSource API), requires no WebSocket upgrade negotiation, and works through HTTP proxies and load balancers without special configuration.</li> </ul>"},{"location":"adr/002-agent-server-protocol/#why-not-grpc","title":"Why not gRPC?","text":"<ul> <li>gRPC adds proto compilation to the build process and requires generated code in both agent and server.</li> <li>Streaming gRPC connections need keepalive management, reconnection logic, and buffering for backpressure.</li> <li>For payloads under 10 KB at 2-second intervals, the overhead of HTTP connection setup is negligible (and HTTP/1.1 keep-alive eliminates it entirely).</li> <li>gRPC debugging requires <code>grpcurl</code> or similar tools rather than standard <code>curl</code>.</li> </ul> <p>gRPC would be reconsidered if the payload size or frequency increased significantly (e.g., sub-second reporting with hundreds of concurrent pulls per node).</p>"},{"location":"adr/002-agent-server-protocol/#consequences","title":"Consequences","text":"<ul> <li>The server must handle potentially bursty POST traffic if many nodes report simultaneously. This is mitigated by the small payload size and the ability to scale server replicas.</li> <li>There is a latency floor equal to the report interval (default 2 seconds). Pull progress updates are not instantaneous but are sufficient for human-readable UIs.</li> <li>The full-snapshot model means some bandwidth is used for pulls whose state has not changed. This is acceptable given the small payload size.</li> <li>SSE is unidirectional. If the server needs to send commands to the agent in the future (e.g., \"increase report frequency\"), a separate mechanism would be needed.</li> </ul>"},{"location":"adr/003-ui-technology/","title":"ADR-003: React + Vite Embedded in Go Binary","text":""},{"location":"adr/003-ui-technology/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/003-ui-technology/#date","title":"Date","text":"<p>2025-01-10</p>"},{"location":"adr/003-ui-technology/#context","title":"Context","text":"<p>Pulltrace needs a web UI that displays live-updating image pull progress. The UI requirements are:</p> <ul> <li>Real-time updates via SSE (Server-Sent Events).</li> <li>Expandable rows showing per-layer progress within each image pull.</li> <li>Filtering and sorting of active pulls.</li> <li>Progress bars with download speed and ETA.</li> <li>Pod correlation display.</li> <li>Works as a single deployment unit (no separate frontend hosting).</li> </ul> <p>We evaluated three approaches:</p> <ol> <li>React + Vite -- SPA built with Vite, embedded into the Go server binary using <code>embed.FS</code>.</li> <li>HTMX + server-rendered HTML -- Server generates HTML fragments, HTMX handles partial updates.</li> <li>Vanilla JavaScript -- No framework, plain DOM manipulation.</li> </ol>"},{"location":"adr/003-ui-technology/#decision","title":"Decision","text":"<p>Use React 18 with Vite for the web UI. The built assets are embedded into the Go server binary using Go's <code>embed</code> package, producing a single self-contained binary.</p>"},{"location":"adr/003-ui-technology/#build-pipeline","title":"Build pipeline","text":"<ol> <li><code>cd web &amp;&amp; npm run build</code> produces static assets in <code>web/dist/</code>.</li> <li>The Go server uses <code>//go:embed web/dist</code> to include these assets at compile time.</li> <li>The server serves the embedded assets at <code>/</code> and the API at <code>/api/</code>.</li> </ol>"},{"location":"adr/003-ui-technology/#development-workflow","title":"Development workflow","text":"<ul> <li><code>cd web &amp;&amp; npm run dev</code> starts Vite's dev server with hot reload.</li> <li>Vite proxies <code>/api</code> requests to <code>localhost:8080</code> (the Go server) during development.</li> </ul>"},{"location":"adr/003-ui-technology/#rationale","title":"Rationale","text":"<ul> <li>React is widely known. Contributors are more likely to be familiar with React than with HTMX or other alternatives. This lowers the barrier to contribution.</li> <li>Vite builds fast. Production builds complete in under 5 seconds. The dev server provides instant hot module replacement.</li> <li>Complex client-side state. The UI manages SSE connections, filtering state, expanded/collapsed rows, computed rates, and ETA calculations. React's component model and hooks (<code>useState</code>, <code>useEffect</code>, <code>useRef</code>) handle this cleanly.</li> <li>Single binary deployment. Embedding the built assets into the Go binary means Pulltrace installs as two container images (agent and server) with no separate frontend service, CDN, or static file server.</li> <li>Minimal dependency footprint. The UI has only two runtime dependencies: <code>react</code> and <code>react-dom</code>. No state management library, CSS framework, or routing library is required.</li> </ul>"},{"location":"adr/003-ui-technology/#why-not-htmx","title":"Why not HTMX?","text":"<p>HTMX was considered seriously because it would eliminate the JavaScript build step entirely. However:</p> <ul> <li>SSE handling with HTMX requires either <code>hx-sse</code> (which replaces entire DOM fragments) or custom JavaScript. The per-layer expandable detail rows and progress bar animations would require significant custom JS regardless.</li> <li>Filtering and sorting on the client side (to avoid server round-trips for every keystroke) would require custom JavaScript state management that HTMX does not provide.</li> <li>The complexity savings from avoiding a JS build step are offset by the complexity of managing interactive state in server-rendered fragments.</li> </ul>"},{"location":"adr/003-ui-technology/#why-not-vanilla-javascript","title":"Why not vanilla JavaScript?","text":"<p>Vanilla JS was rejected because the UI has enough interactive state (dozens of concurrently updating progress bars, expandable sections, filtering) that manual DOM management would be error-prone and hard to maintain. React's declarative model is a better fit.</p>"},{"location":"adr/003-ui-technology/#consequences","title":"Consequences","text":"<ul> <li>Contributors to the UI need Node.js and npm installed for development.</li> <li>The Go build process must run <code>npm run build</code> before compiling the server binary. This is handled in the Makefile and Dockerfile.</li> <li>The embedded UI is static once compiled. UI changes require rebuilding the server binary.</li> <li>React 18 adds approximately 130 KB (gzipped: ~42 KB) to the served assets. This is acceptable for a cluster-internal tool.</li> </ul>"}]}